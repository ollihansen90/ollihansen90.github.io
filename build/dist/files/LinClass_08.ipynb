{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ollihansen90/linclassifiers/blob/main/Futureskills/LinClass_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbPcHvliKy3j"
      },
      "source": [
        "# Kapitel 8 - Perzeptron\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVW6vXBRKsiq"
      },
      "outputs": [],
      "source": [
        "# Dieser Block entfällt im Jupyter-Hub, weil die Daten schon auf dem Server liegt\n",
        "!wget -nc -q https://raw.githubusercontent.com/ollihansen90/linclassifiers/main/Futureskills/utils/utils.py\n",
        "!wget -nc -q https://raw.githubusercontent.com/ollihansen90/linclassifiers/main/Futureskills/utils/utils_linclass_08.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVN7wwH_9lp7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange\n",
        "from utils import generate_data, show_vid\n",
        "from utils_linclass_08 import draw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHyqh08UkILO"
      },
      "source": [
        "In diesem Notebook soll der Lernschritt eines Perzeptrons implementiert werden. Wie im Video bereits erklärt, ist ein Perzeptron ein linearer Klassifizierer mit einer Aktivierungsfunktion, die für eine beliebige (reelle) Eingabe das Vorzeichen zurückgibt.\n",
        "\n",
        "## Daten generieren\n",
        "Die Daten werden genau wie die in Aufgabe 2 von Kapitel 7 generiert. Es handelt sich also um zwei Punktewolke, die sich linear trennen lassen. Die blauen Punkte haben Label `+1`, die orangenen Punkte haben Label `-1`. Beispielhaft wird ebenso eine Gerade eingezeichnet, die das Klassifikationsproblem (noch) nicht löst."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBwNbKJDGyoM"
      },
      "outputs": [],
      "source": [
        "data, label = generate_data()\n",
        "w = np.array([1,1,-1])\n",
        "\n",
        "plt.figure(figsize=[7,7])\n",
        "plt.imshow(draw(w, data, label))\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrnpOSKV94m_"
      },
      "source": [
        "## Update-Schritt\n",
        "Wie im Video bereits erklärt, kann die Klassifikationsgerade schrittweise mit Hilfe der Perzeptron-Lernregel angepasst werden. Hierbei werden die Gewichte, angepasst mit dem Threshold-Trick (also erweitert um `-1`) folgendermaßen aktualisiert: $w_{t+1} = w_t+\\Delta w_t$ mit $\\Delta w_t= \\epsilon\\cdot(s^\\mu-y)\\cdot x$. Die einzelnen Parameter in der Formel haben folgende Bedeutung:\n",
        "- $\\epsilon$: Lernrate, im Code `lr`\n",
        "- $s^\\mu$: Ausgabe des Perzeptrons (also `+1` oder `-1`), im Code `out`\n",
        "- $y$: Label von Punkt $x$ (`+1` oder `-1`), im Code `label`\n",
        "- $x$: Punkt, der in das Perzeptron gegeben wird, im Code `data`.\n",
        "\n",
        "Da es sich bei `data` und `label` um lange Listen handelt, werden diese eintragsweise verarbeitet, indem ein Index durch die Liste läuft. Der `i`-te Eintrag wäre hier `data[i]`, bzw. `label[i]`.\n",
        "\n",
        "Die Aktivierungsfunktion, die das Vorzeichen einer Eingabe zurückgibt, wurde über eine `if`-Abfrage implementiert.\n",
        "\n",
        "Sind Label und Ausgabe des Perzeptrons gleich, so gibt es keine Veränderung der Gewichte, da $\\Delta w_t= \\epsilon\\cdot(1-1)\\cdot x=0$ bzw. $\\Delta w_t= \\epsilon\\cdot(-1-(-1))\\cdot x=0$ gilt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rTEuSJPIWO9"
      },
      "outputs": [],
      "source": [
        "datal, label = generate_data()\n",
        "\n",
        "# --- Gewichte und Lernrate anpassen ---\n",
        "w = np.array(\n",
        "        [4,-4,-1]\n",
        "    )\n",
        "lr = 0.001\n",
        "# -------------------------------------\n",
        "\n",
        "vid = [draw(w, data, label)]\n",
        "\n",
        "for i in trange(len(data)):\n",
        "    out = data[i]@w # Skalarprodukt des Datenpunktes mit dem Gewichtsvektor\n",
        "\n",
        "    # Vorzeichenfunktion\n",
        "    if out>=0:\n",
        "        out = 1\n",
        "    else:\n",
        "        out = -1\n",
        "\n",
        "    # Berechne Updateschritt\n",
        "    dw = lr*(label[i]-out)*data[i]\n",
        "    # Wende Updateschritt an\n",
        "    w = w+dw\n",
        "\n",
        "    # Überspringe alle Bilder, in denen sich nichts ändert\n",
        "    if out == label[i]:\n",
        "        continue\n",
        "    image = draw(w, data, label)\n",
        "    vid.append(image)\n",
        "\n",
        "show_vid(vid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZW_xiVQLmIc"
      },
      "source": [
        "## Update über Mehrere Epochen\n",
        "Üblicherweise wird ein Perzeptron über mehrere Epochen trainiert, da das Klassifikationsproblem oftmals nach nur einer Epoche nicht gelöst ist. \"Epoche\" ist hier so definiert, dass jeder Datenpunkt einmal betrachtet wurde und entsprechend ein Updateschritt durchgeführt wurde.\n",
        "\n",
        "Die Funktion `update_epoch` führt den Updateschritt für eine Epoche durch, ohne die Abbildungen für jeden Updateschritt zu erstellen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgMGa8xf_67x"
      },
      "outputs": [],
      "source": [
        "def update_epoch(w, lr, data, label):\n",
        "    # Durchlaufe den Index jedes Datenpunktes\n",
        "    for i in range(len(data)):\n",
        "        out = data[i]@w # Skalarprodukt des Datenpunktes mit dem Gewichtsvektor\n",
        "\n",
        "        # Vorzeichenfunktion\n",
        "        if out>=0:\n",
        "            out = 1\n",
        "        else:\n",
        "            out = -1\n",
        "\n",
        "        # Berechne Updateschritt\n",
        "        dw = lr*(label[i]-out)*data[i]\n",
        "        # Wende Updateschritt an\n",
        "        w = w+dw\n",
        "    return w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOy1DIwUNT6F"
      },
      "source": [
        "## Perzeptron-Training\n",
        "Im letzten Abschnitt soll nun alles zusammengesetzt werden: Ein Perzeptron wird definiert, das das gegebene Klassifikationsproblem nicht löst. Schrittweise werden die Gewichte jetzt so angepasst, dass ein Klassifikator entsteht, der die beiden Klassen korrekt voneinander trennt.\n",
        "\n",
        "Im unteren Abschnitt können Hyperparameter festgelegt werden. Diese Hyperparameter sind die *Lernrate* `lr`, eine *Toleranz* `tol`, sowie eine *maximale Anzahl an Schritten* `n_max`.\n",
        "\n",
        "Die Toleranz und die Maximalzahl an Schritten definieren eine Abbruchbedingung des Trainings. Nur in den seltensten Fällen gibt es Problemstellungen, die linear trennbar sind. Wie oben beobachtet werden konnte, gibt es Updateschritte, solange es noch falsche Klassifikationen gibt. Sind die Klassen nicht linear trennbar, so würde das Training ewig weitergeführt werden.\n",
        "\n",
        "Die einfachste Abbruchbedingung ist eine maximale Anzahl an Schritten. Sobald diese Zahl erreicht wird, soll das Training beendet werden. Hierbei ist es unerheblich, wie gut das Problem zu dem Zeitpunkt gelöst ist.\n",
        "\n",
        "Eine weitere Abbruchbedingung ist die Toleranz. Die Toleranz kann sehr unterschiedlich definiert sein. Im folgenden Codeblock ist sie so definiert, dass das Training abgebrochen wird, sobald sich die Gewichte von einer Epoche zur nächsten nicht mehr stark ändern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOg2Y1C_EqdG"
      },
      "outputs": [],
      "source": [
        "def run(lr,n_max=50, tol=1e-5):\n",
        "    # Generiere Daten\n",
        "    data, label = generate_data()\n",
        "\n",
        "    # Initialisiere Gewichte w\n",
        "    w = np.array([1,-1,1])\n",
        "\n",
        "    # Initialisiere Video mit dem Startbild\n",
        "    vid = [draw(w, data, label)]\n",
        "\n",
        "    # Training\n",
        "    for epoch in trange(n_max):\n",
        "        w_alt = w.copy()\n",
        "\n",
        "        # Update der Gewichte\n",
        "        w = update_epoch(w, lr, data, label)\n",
        "\n",
        "        # Einzeichnen der Punkte und des Perzeptrons\n",
        "        image = draw(w, data, label)\n",
        "        vid.append(image)\n",
        "\n",
        "        # Abbruch bei wenig Veränderung\n",
        "        if np.linalg.norm(w-w_alt)<tol:\n",
        "            break\n",
        "    show_vid(vid)\n",
        "\n",
        "# --- Hier können die Hyperparameter erstellt werden ---\n",
        "lernrate = 0.0001\n",
        "tol = 0.00001\n",
        "n_max = 50\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "run(lr=lernrate, n_max=n_max, tol=tol)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPIAxm4uG6tkhC1urONOI7t",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
